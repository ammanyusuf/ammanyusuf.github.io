<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ammanyusuf.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ammanyusuf.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-31T20:36:35+00:00</updated><id>https://ammanyusuf.github.io/feed.xml</id><title type="html">Amman Yusuf</title><subtitle>Amman Yusuf is a machine learning engineer and researcher working across machine learning and software engineering </subtitle><entry><title type="html">Emotion in Motion</title><link href="https://ammanyusuf.github.io/blog/2024/emotion-in-motion/" rel="alternate" type="text/html" title="Emotion in Motion"/><published>2024-01-04T00:00:00+00:00</published><updated>2024-01-04T00:00:00+00:00</updated><id>https://ammanyusuf.github.io/blog/2024/emotion-in-motion</id><content type="html" xml:base="https://ammanyusuf.github.io/blog/2024/emotion-in-motion/"><![CDATA[<h2 id="key-takeaways">Key takeaways</h2> <p>We explored the intersection of robotics, emotions, and human-robot interaction through a user elicitation study with Sony Toio robots. Participants generated motions to convey emotion, and we analyzed distance and speed features to extract patterns. The project strengthened my UI/UX research skills and highlighted how motion can bridge emotional communication for non-anthropomorphic robots.</p> <h2 id="paper">Paper</h2> <p>Download the draft manuscript: <a href="/assets/pdf/emotion-in-motion-manuscript.pdf">Emotion in Motion manuscript</a></p> <h2 id="abstract">Abstract</h2> <p>Non-anthropomorphic robots lack human-like expressions, so motion becomes a key communication channel. This study focused on user-defined motions and the emotions those motions convey. Participants controlled robots by hand and created movements for eight emotions. The resulting patterns offer insights into designing more expressive robots for social settings.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/articles/teaser-figure-480.webp 480w,/assets/img/articles/teaser-figure-800.webp 800w,/assets/img/articles/teaser-figure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/articles/teaser-figure.png" width="100%" height="auto" style=" max-width: 640px; " title="Emotion in Motion teaser figure." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="methodology">Methodology</h2> <p>We ran ideation sessions and recorded motion data. Plutchik’s model guided emotion selection, expanding beyond Ekman’s six basic emotions. We extracted features (distance, speed) and analyzed patterns across emotions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/articles/plutchik-wheel-480.webp 480w,/assets/img/articles/plutchik-wheel-800.webp 800w,/assets/img/articles/plutchik-wheel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/articles/plutchik-wheel.png" width="100%" height="auto" style=" max-width: 640px; " title="Plutchik wheel for emotion modeling." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="results">Results</h2> <p>Distinct movement patterns emerged for different emotions. These patterns can inform motion-based emotional expressions in non-anthropomorphic robots.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/articles/user-defined-motions-480.webp 480w,/assets/img/articles/user-defined-motions-800.webp 800w,/assets/img/articles/user-defined-motions-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/articles/user-defined-motions.png" width="100%" height="auto" style=" max-width: 640px; " title="User-defined motion patterns." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <p>By uncovering relationships between movement and emotional perception, we can design better interactions between people and robots.</p> <p>Person at work … check back later.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/articles/freetze-480.webp 480w,/assets/img/articles/freetze-800.webp 800w,/assets/img/articles/freetze-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/articles/freetze.jpg" width="100%" height="auto" style=" max-width: 320px; " title="Person at work." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="articles"/><category term="research"/><category term="hri"/><category term="hci"/><summary type="html"><![CDATA[Can a robot feel? Not yet. But how can motion convey emotion in non-anthropomorphic robots?]]></summary></entry><entry><title type="html">Federated Learning and Its Applications</title><link href="https://ammanyusuf.github.io/blog/2024/federated-learning-applications/" rel="alternate" type="text/html" title="Federated Learning and Its Applications"/><published>2024-01-04T00:00:00+00:00</published><updated>2024-01-04T00:00:00+00:00</updated><id>https://ammanyusuf.github.io/blog/2024/federated-learning-applications</id><content type="html" xml:base="https://ammanyusuf.github.io/blog/2024/federated-learning-applications/"><![CDATA[<h2 id="key-takeaways">Key takeaways</h2> <p>This project was an immersive exploration of deep learning, computer vision, federated learning, and machine learning. I implemented and evaluated a federated learning pipeline using a U-Net model for wheat head instance segmentation. I trained models on ARC remote servers, worked on image processing and transformation techniques, and ran a literature review that sharpened my research toolkit.</p> <h2 id="paper">Paper</h2> <p>Download the draft manuscript: <a href="/assets/pdf/fl-draft-manuscript.pdf">FL draft manuscript</a></p> <h2 id="abstract">Abstract</h2> <p>In my final year thesis project at the University of Calgary, we tackled the challenges associated with centralized learning in wheat head instance segmentation, specifically addressing privacy concerns and computational limitations. The goal was to improve accuracy and efficiency in this agricultural task by leveraging federated learning, an approach that improves data privacy and avoids centralized bottlenecks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/articles/fl-representation-480.webp 480w,/assets/img/articles/fl-representation-800.webp 800w,/assets/img/articles/fl-representation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/articles/fl-representation.png" width="100%" height="auto" style=" max-width: 640px; " title="Federated learning representation diagram." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="federated-learning-solution">Federated learning solution</h2> <p>Federated learning emerged as a solution to these challenges, and our project focused on implementing and evaluating a federated learning pipeline for wheat head instance segmentation. The pipeline was model-agnostic and used a modified U-Net to compare approaches across clients.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/articles/unet-architecture-480.webp 480w,/assets/img/articles/unet-architecture-800.webp 800w,/assets/img/articles/unet-architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/articles/unet-architecture.png" width="100%" height="auto" style=" max-width: 640px; " title="Modified U-Net architecture." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="comparison-of-approaches">Comparison of approaches</h2> <p>We compared two approaches - FedAvg and FedProx - and evaluated using Intersection over Union and Dice scores, against a centralized baseline. Wheat head images from 18 institutions were sourced from the Global Wheat Head Dataset. FedAvg achieved a Dice score of 0.9355 on the test set, outperforming the centralized baseline.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/articles/segmented-examples-480.webp 480w,/assets/img/articles/segmented-examples-800.webp 800w,/assets/img/articles/segmented-examples-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/articles/segmented-examples.png" width="100%" height="auto" style=" max-width: 640px; " title="Example segmentations from the wheat head dataset." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <p>This project shows the potential of federated learning to address privacy and scalability challenges in agricultural computer vision. It is a practical path forward for applying ML where data access is constrained.</p> <p>Person at work … check back later.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/articles/freetze-480.webp 480w,/assets/img/articles/freetze-800.webp 800w,/assets/img/articles/freetze-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/articles/freetze.jpg" width="100%" height="auto" style=" max-width: 320px; " title="Person at work." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="articles"/><category term="research"/><category term="federated-learning"/><category term="computer-vision"/><summary type="html"><![CDATA[Federated learning is a growing field in AI, with clear benefits for privacy and performance.]]></summary></entry></feed>